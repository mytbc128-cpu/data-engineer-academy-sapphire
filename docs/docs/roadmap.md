
# 🚀 Data Engineer Academy — Sapphire Roadmap

**Author:** Michael Fuentes
**Program Duration:** 13 Weeks (9 Hours per Week)
**Goal:** To become a professional data engineer capable of building end-to-end pipelines, data warehouses, and analytics systems — and to document every step of the journey publicly on GitHub.

---

## 💎 The Sapphire Vision

This program is not just a technical course — it’s a transformation journey.
Each week, I’m crafting the skills, mindset, and projects that form the foundation of a noble, calm, and wise engineer — one who builds systems that empower data-driven decisions.

The Sapphire symbolizes **clarity, wisdom, and structure** — the same traits I aim to bring into my code, architecture, and problem-solving.

---

## 🧭 Learning Path Overview

| Week | Theme | Focus Area | Deliverable |
|------|--------|-------------|--------------|
| 00 | Setup | Environment, GitHub, Python | Local environment ready |
| 01 | SQL Basics | SELECT, WHERE, ORDER BY | Practice queries |
| 02 | Advanced SQL | Aggregations, Window functions | Complex reports |
| 03 | Python for Data | Pandas, CSVs, scripting | Data cleaning script |
| 04 | Data Cleaning | Missing data, formats | Clean dataset |
| 05 | API + CSV ETL | Extract, Transform, Load | ETL pipeline in Python |
| 06 | Database Design | ERD + Normalization | Database schema |
| 07 | Data Warehouse | Star schema, dimensions/facts | SQL warehouse |
| 08 | dbt Transformations | Staging + Marts | dbt models |
| 09 | Airflow | DAG scheduling | Automated pipeline |
| 10 | Cloud Deployment | AWS S3 + Redshift | Cloud ETL |
| 11 | Streaming Data | Kafka demo | Real-time pipeline |
| 12 | Dashboards | Power BI or Tableau | Visualization |
| 13 | Capstone Project | Full E-commerce Data Pipeline | Final portfolio project |

---

## 🧱 Weekly Structure

Each week is divided into 3 focus blocks:
1. **Learn (3 hours)** — tutorials, reading, or guided lessons.
2. **Build (5 hours)** — hands-on projects and scripts committed to GitHub.
3. **Reflect (1 hour)** — document lessons, challenges, and breakthroughs.

---

## 🗓️ Weekly Breakdown

### Week 00 — Setup
- Install Python, Git, and Postgres
- Create the `data-engineer-academy-sapphire` repo
- Learn GitHub basics and push first commit

### Week 01 — SQL Basics
- Practice with SELECT, WHERE, ORDER BY
- Build sample employee or e-commerce table
- Upload `.sql` exercises to `week01_sql_basics/`

### Week 02 — Advanced SQL
- GROUP BY, HAVING, JOIN, SUBQUERY, WINDOW FUNCTIONS
- Create `advanced_queries.sql`
- Document insights in `README.md`

### Week 03 — Python for Data
- Learn Pandas, CSV import/export
- Write a script to clean messy CSV data
- Save in `week03_python_for_data/`

### Week 04 — Data Cleaning
- Handle missing values, duplicates, and types
- Commit a cleaned dataset with before/after notes

### Week 05 — API + CSV ETL
- Write Python script to pull data from API and save to Postgres
- Automate it using cron or schedule

### Week 06 — Database Design
- Design ER Diagram for e-commerce
- Create SQL schema file: `ecommerce_schema.sql`

### Week 07 — Data Warehouse
- Create star schema: `fact_orders`, `dim_customers`, etc.
- Practice warehouse queries

### Week 08 — dbt Transformations
- Initialize dbt project
- Build staging and mart models
- Add documentation and dbt tests

### Week 09 — Airflow Orchestration
- Install Airflow locally
- Build a DAG to run ETL + dbt daily

### Week 10 — Cloud AWS S3 + Redshift
- Upload sample data to S3
- Query it from Redshift or Athena
- Document cloud setup steps

### Week 11 — Streaming (Kafka)
- Create local Kafka topic
- Producer sends fake order data
- Consumer logs to console or DB

### Week 12 — Dashboard (Power BI / Tableau)
- Connect to warehouse
- Build sales performance dashboard
- Screenshot and save in `week12_dashboard_powerbi/`

### Week 13 — Capstone Project
- Integrate all: ingestion → warehouse → dashboard
- Present in `week13_capstone_project/`
- Write a final reflection blog or Medium article

---

## 🏆 Capstone Goal

**Project Title:** *E-commerce Data Warehouse Pipeline*
**Tech Stack:** Python, Postgres, dbt, Airflow, AWS, Power BI
**Output:**
- Ingest CSV/API → Postgres
- Transform using dbt
- Schedule in Airflow
- Visualize KPIs in BI tool

All code, diagrams, and reflections will live inside this repository.

---

## 🌱 Mindset Notes

> “Every commit is a step toward mastery.”
> Each push to GitHub is not about perfection, but **progress**.
> By documenting openly, I’m building discipline, consistency, and visibility.

I’m not just learning tools — I’m sculpting a professional identity.
This is the Sapphire Journey: clarity in code, wisdom in process, and calmness in learning.

---

## 💬 Future Plans
After Week 13:
- Contribute to open-source data engineering projects
- Deploy real pipelines on AWS
- Start applying for **Data Engineer** roles with confidence

---

### 🟦 Progress Tracker

| Week | Status | Date |
|------|---------|------|
| 00 | ✅ Completed | |
| 01 | 🔄 In Progress | |
| 02 | ⏳ Upcoming | |
| ... | ... | ... |
| 13 | 🎯 Capstone | |

---

**Created with purpose and passion by Michael Fuentes**
*#SapphireJourney — A 13-week transformation into a Data Engineer*
