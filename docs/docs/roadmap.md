
# ğŸš€ Data Engineer Academy â€” Sapphire Roadmap

**Author:** Michael Fuentes
**Program Duration:** 13 Weeks (9 Hours per Week)
**Goal:** To become a professional data engineer capable of building end-to-end pipelines, data warehouses, and analytics systems â€” and to document every step of the journey publicly on GitHub.

---

## ğŸ’ The Sapphire Vision

This program is not just a technical course â€” itâ€™s a transformation journey.
Each week, Iâ€™m crafting the skills, mindset, and projects that form the foundation of a noble, calm, and wise engineer â€” one who builds systems that empower data-driven decisions.

The Sapphire symbolizes **clarity, wisdom, and structure** â€” the same traits I aim to bring into my code, architecture, and problem-solving.

---

## ğŸ§­ Learning Path Overview

| Week | Theme | Focus Area | Deliverable |
|------|--------|-------------|--------------|
| 00 | Setup | Environment, GitHub, Python | Local environment ready |
| 01 | SQL Basics | SELECT, WHERE, ORDER BY | Practice queries |
| 02 | Advanced SQL | Aggregations, Window functions | Complex reports |
| 03 | Python for Data | Pandas, CSVs, scripting | Data cleaning script |
| 04 | Data Cleaning | Missing data, formats | Clean dataset |
| 05 | API + CSV ETL | Extract, Transform, Load | ETL pipeline in Python |
| 06 | Database Design | ERD + Normalization | Database schema |
| 07 | Data Warehouse | Star schema, dimensions/facts | SQL warehouse |
| 08 | dbt Transformations | Staging + Marts | dbt models |
| 09 | Airflow | DAG scheduling | Automated pipeline |
| 10 | Cloud Deployment | AWS S3 + Redshift | Cloud ETL |
| 11 | Streaming Data | Kafka demo | Real-time pipeline |
| 12 | Dashboards | Power BI or Tableau | Visualization |
| 13 | Capstone Project | Full E-commerce Data Pipeline | Final portfolio project |

---

## ğŸ§± Weekly Structure

Each week is divided into 3 focus blocks:
1. **Learn (3 hours)** â€” tutorials, reading, or guided lessons.
2. **Build (5 hours)** â€” hands-on projects and scripts committed to GitHub.
3. **Reflect (1 hour)** â€” document lessons, challenges, and breakthroughs.

---

## ğŸ—“ï¸ Weekly Breakdown

### Week 00 â€” Setup
- Install Python, Git, and Postgres
- Create the `data-engineer-academy-sapphire` repo
- Learn GitHub basics and push first commit

### Week 01 â€” SQL Basics
- Practice with SELECT, WHERE, ORDER BY
- Build sample employee or e-commerce table
- Upload `.sql` exercises to `week01_sql_basics/`

### Week 02 â€” Advanced SQL
- GROUP BY, HAVING, JOIN, SUBQUERY, WINDOW FUNCTIONS
- Create `advanced_queries.sql`
- Document insights in `README.md`

### Week 03 â€” Python for Data
- Learn Pandas, CSV import/export
- Write a script to clean messy CSV data
- Save in `week03_python_for_data/`

### Week 04 â€” Data Cleaning
- Handle missing values, duplicates, and types
- Commit a cleaned dataset with before/after notes

### Week 05 â€” API + CSV ETL
- Write Python script to pull data from API and save to Postgres
- Automate it using cron or schedule

### Week 06 â€” Database Design
- Design ER Diagram for e-commerce
- Create SQL schema file: `ecommerce_schema.sql`

### Week 07 â€” Data Warehouse
- Create star schema: `fact_orders`, `dim_customers`, etc.
- Practice warehouse queries

### Week 08 â€” dbt Transformations
- Initialize dbt project
- Build staging and mart models
- Add documentation and dbt tests

### Week 09 â€” Airflow Orchestration
- Install Airflow locally
- Build a DAG to run ETL + dbt daily

### Week 10 â€” Cloud AWS S3 + Redshift
- Upload sample data to S3
- Query it from Redshift or Athena
- Document cloud setup steps

### Week 11 â€” Streaming (Kafka)
- Create local Kafka topic
- Producer sends fake order data
- Consumer logs to console or DB

### Week 12 â€” Dashboard (Power BI / Tableau)
- Connect to warehouse
- Build sales performance dashboard
- Screenshot and save in `week12_dashboard_powerbi/`

### Week 13 â€” Capstone Project
- Integrate all: ingestion â†’ warehouse â†’ dashboard
- Present in `week13_capstone_project/`
- Write a final reflection blog or Medium article

---

## ğŸ† Capstone Goal

**Project Title:** *E-commerce Data Warehouse Pipeline*
**Tech Stack:** Python, Postgres, dbt, Airflow, AWS, Power BI
**Output:**
- Ingest CSV/API â†’ Postgres
- Transform using dbt
- Schedule in Airflow
- Visualize KPIs in BI tool

All code, diagrams, and reflections will live inside this repository.

---

## ğŸŒ± Mindset Notes

> â€œEvery commit is a step toward mastery.â€
> Each push to GitHub is not about perfection, but **progress**.
> By documenting openly, Iâ€™m building discipline, consistency, and visibility.

Iâ€™m not just learning tools â€” Iâ€™m sculpting a professional identity.
This is the Sapphire Journey: clarity in code, wisdom in process, and calmness in learning.

---

## ğŸ’¬ Future Plans
After Week 13:
- Contribute to open-source data engineering projects
- Deploy real pipelines on AWS
- Start applying for **Data Engineer** roles with confidence

---

### ğŸŸ¦ Progress Tracker

| Week | Status | Date |
|------|---------|------|
| 00 | âœ… Completed | |
| 01 | ğŸ”„ In Progress | |
| 02 | â³ Upcoming | |
| ... | ... | ... |
| 13 | ğŸ¯ Capstone | |

---

**Created with purpose and passion by Michael Fuentes**
*#SapphireJourney â€” A 13-week transformation into a Data Engineer*
